{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Notebook/adviewRecomm/version3/temp/\n"
     ]
    }
   ],
   "source": [
    "## import data processing/cleaning , data modeling libraries\n",
    "#!pip install boto3\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import re as re\n",
    "import datetime as datetime\n",
    "import numpy as np\n",
    "import collections\n",
    "import boto3\n",
    "import io\n",
    "import string\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import silhouette_score\n",
    "    \n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models import doc2vec\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import fasttext\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "source=\"/home/ubuntu/Notebook/adviewRecomm/version3/temp/\"\n",
    "sparkfiles = \"/home/ubuntu/Notebook/spark-modules/clust_vtitleTrans/\"\n",
    "destination=source\n",
    "\n",
    "lang=[\"en\"] # video language got from video title using python Library\n",
    "clustSize = 0 # no of clusters - initialization\n",
    "t0 = datetime.datetime.now()\n",
    "print(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(215482, 32)\n",
      "                v_id                          v_title  \\\n",
      "0  10155398318101089  How it's made Electrical Cables   \n",
      "1  10155398314226089       How It's Made Racing Karts   \n",
      "\n",
      "                     v_title_trans ch_lang_raw  Unnamed: 0  ch_id v_descr  \\\n",
      "0  How it's made Electrical Cables     english         NaN    NaN     NaN   \n",
      "1       How It's Made Racing Karts     english         NaN    NaN     NaN   \n",
      "\n",
      "   v_seq_id v_lang_raw v_thumbnail    ...      ch_is_clickable v_stat_viewCnt  \\\n",
      "0       NaN        NaN         NaN    ...                  NaN            NaN   \n",
      "1       NaN        NaN         NaN    ...                  NaN            NaN   \n",
      "\n",
      "  v_stat_likeCnt  v_stat_dislikeCnt v_stat_cmntCnt v_stat_status  \\\n",
      "0            NaN                NaN            NaN           NaN   \n",
      "1            NaN                NaN            NaN           NaN   \n",
      "\n",
      "  v_stat_updatStamp lang_vtitle videoPubDate videoPubYr  \n",
      "0               NaN         NaN          NaN        NaN  \n",
      "1               NaN         NaN          NaN        NaN  \n",
      "\n",
      "[2 rows x 32 columns]\n",
      "(215482, 32)\n"
     ]
    }
   ],
   "source": [
    "## outData41 - video data coming from postgres DB\n",
    "#  outData41.to_csv(source+\"youtubeModelData_stats1.csv\",index=False)\n",
    "\n",
    "outData43 = pd.read_csv(source+\"clipVdChanelData_model_in.csv\",encoding=\"utf-8\")\n",
    "print(outData43.shape)\n",
    "print(outData43.head(n=2)) # 39:\n",
    "outData43 = outData43.drop_duplicates()\n",
    "outData43 = outData43.dropna(subset=[\"v_title_trans\"])\n",
    "print(outData43.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rukh',\n",
       " 'marianne',\n",
       " 'full',\n",
       " 'india',\n",
       " 'salim',\n",
       " 'singh',\n",
       " 'chak',\n",
       " 'title',\n",
       " 'song',\n",
       " 'shah',\n",
       " 'cruz',\n",
       " 'sukhvinder',\n",
       " 'khan']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## NLP PRE-PROCESSING\n",
    "\n",
    "#text=\" the JanaSena Party Formation Day celebrations || LIVE funny celebration|| Pawan Kalyan || Guntur\"\n",
    "text=\"Chak De India | Full Title Song | Shah Rukh Khan | Sukhvinder Singh | Salim | Marianne D'Cruz\"\n",
    "\n",
    "def nltk_clean_sent(line):\n",
    "    if len(line)>0:\n",
    "        ## remove the punctuation/emoticons/digits/multispaces with single from the line\n",
    "        ## dont make lowercase before the pos tagging\n",
    "        line_lower = line.strip()\n",
    "        line_punct = re.sub('['+string.punctuation+']',' ',line_lower)\n",
    "        line_emots = re.sub(r'[\\u200b-\\u2fff]+',' ',line_punct)\n",
    "        line_digis = re.sub(r'[0-9]+',' ',line_emots)\n",
    "        line_spaces = re.sub(r'[\\s]+',' ',line_digis)\n",
    "        line = line_spaces\n",
    "    return line\n",
    "\n",
    "def nltk_extract_postags(line):\n",
    "    cleaned_str = ''\n",
    "    tokens = nltk.word_tokenize(line)\n",
    "    tokens_pos = PerceptronTagger().tag(tokens)\n",
    "    #print(tokens_pos)\n",
    "    # noun tags\n",
    "    nn_tags = ['NN', 'NNP', 'NNP', 'NNPS', 'NNS']\n",
    "    # adjectives\n",
    "    jj_tags = ['JJ', 'JJR', 'JJS']\n",
    "    # verbs\n",
    "    vb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "    nltk_tags = nn_tags + jj_tags + vb_tags\n",
    "    \n",
    "    for tag_word in tokens_pos:\n",
    "        if tag_word[1] in nltk_tags:\n",
    "            if len(tag_word[0])>2:\n",
    "                cleaned_str += tag_word[0]+' '\n",
    "    return cleaned_str.strip().lower()\n",
    "\n",
    "def nltk_apply_lemma(line):\n",
    "    tokens_lemmas = [WordNetLemmatizer().lemmatize(word) for word in line.split()]\n",
    "    ## stemming\n",
    "    tokens_stops = [word for word in tokens_lemmas if word not in stopwords.words('english')]\n",
    "    tokens_stops = [word for word in tokens_stops if len(word.strip())>2]\n",
    "    tokens_stops = list(set(tokens_stops))\n",
    "    return tokens_stops\n",
    "\n",
    "def nltk_extract_tags(line):\n",
    "        ## tokenize the sentence/get tokens that contains only letters\n",
    "        line_clean = nltk_clean_sent(line)\n",
    "        ## apply postags to the words and get only couple of tags and word length >2\n",
    "        tokens_pos = nltk_extract_postags(line_clean)\n",
    "        ## apply lemmatize/stemming and remove stopwords\n",
    "        token_lemma = nltk_apply_lemma(tokens_pos)\n",
    "        return token_lemma\n",
    "    \n",
    "nltk_extract_tags(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TF-IDF transformation\n",
    "def getTfIdfMetrics(outData44,lang):\n",
    "    outData44_en = outData44[outData44.ch_lang_raw==lang]\n",
    "    t0 = datetime.datetime.now()\n",
    "    tfidf_transform = TfidfVectorizer(tokenizer=nltk_extract_tags,min_df=6,max_df=0.9,stop_words='english',use_idf=True,ngram_range=(1,1))\n",
    "    #terms = tfidf_transform.get_feature_names()\n",
    "    #print(len(terms))\n",
    "    #print(terms[:100])\n",
    "    tfidf_vecto = tfidf_transform.fit_transform(outData44_en[\"v_title_trans\"])\n",
    "    print(tfidf_vecto.shape)\n",
    "    # got shape of (83806, 149542) - without translation\n",
    "    t1 = datetime.datetime.now()\n",
    "    print(\"END TIME after TFIDF =\",t1)\n",
    "    print(\"time taken until TFIDF=\",(t1-t0))\n",
    "    return tfidf_vecto\n",
    "\n",
    "## PRINTING FIRST 100 FEATURES\n",
    "#getTfIdfMetrics(outData44,'telugu')\n",
    "#terms = tfidf_transform.get_feature_names()\n",
    "#print(terms[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ## DOC2VEC model\n",
    "# #outData44_en_sample = outData44_en[50:55]\n",
    "\n",
    "# cleaneddoc = [nltk_extract_tags(text) for text in list(outData44_en[\"v_title_trans\"])]\n",
    "# taggeddoc = [doc2vec.TaggedDocument(val,[idx]) for idx,val in enumerate(cleaneddoc)]\n",
    "\n",
    "# d2vmodel = Doc2Vec(size=100,min_count=2,alpha=0.025,min_alpha=0.025)\n",
    "# d2vmodel.build_vocab(taggeddoc)\n",
    "# #print(d2vmodel[0])\n",
    "# d2vmodel.train(taggeddoc,total_examples=d2vmodel.corpus_count,epochs=10,start_alpha=0.002,end_alpha=-0.016)\n",
    "# d2vmodel_vecs = [d2vmodel.infer_vector(val) for idx,val in enumerate(cleaneddoc)]\n",
    "# #print(d2vmodel_vecs[0])\n",
    "\n",
    "##feature_vecto = d2vmodel.docvecs.doctag_syn0\n",
    "##print(len(d2vmodel_vecs))\n",
    "\n",
    "## WORD2VEC model # Word2Vec\n",
    "def getWord2VecMetrics(outData44,lang):\n",
    "    outData44_en = outData44[outData44.ch_lang_raw==lang]\n",
    "    t0 = datetime.datetime.now()\n",
    "    cleaneddoc = [nltk_extract_tags(text) for text in list(outData44_en[\"v_title_trans\"])]\n",
    "    cleaneddoc = [text for text in cleaneddoc if len(text)>0]\n",
    "    w2vmodel = Word2Vec(cleaneddoc,min_count=1,size=200)\n",
    "    w2v_vecs = w2vmodel.wv.syn0\n",
    "    t1 = datetime.datetime.now()\n",
    "    print(\"END TIME after TFIDF =\",t1)\n",
    "    print(\"time taken until TFIDF=\",(t1-t0))\n",
    "    return w2v_vecs\n",
    "#feature_vecto = getWord2VecMetrics(outData44,lang)\n",
    "#print(feature_vecto.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## MODEL FUNCTION DEFINITION\n",
    "#feature_vecto = np.stack(d2vmodel.docvecs)\n",
    "def getModelDataTfIdf(outData5,lang,feature_vecto):\n",
    "    ## chosing no of clusters\n",
    "    if len(outData5) < 1800:\n",
    "        clustSize=3\n",
    "    else:\n",
    "        clustSize = int(np.round(np.divide(len(outData5),1800)))\n",
    "    print(\"cluster lang=\",lang)    \n",
    "    print(\"clust data len=\",len(outData5))\n",
    "    print(\"cluster size=\",clustSize)\n",
    "\n",
    "    ## KMeans model\n",
    "    from sklearn.cluster import KMeans\n",
    "    #kmeans = KMeans(n_clusters=clustSize,init='k-means++',max_iter=80).fit(feature_vecto)\n",
    "    kmeans = KMeans(n_clusters=clustSize).fit(feature_vecto)\n",
    "    preds1 = kmeans.labels_\n",
    "\n",
    "    ### NO worries on this PART ###\n",
    "    print(collections.Counter(preds1))\n",
    "    b=pd.DataFrame(preds1)\n",
    "    clustData1=pd.DataFrame(outData5)\n",
    "    clustData1[\"clustNum\"]=pd.Series(preds1)\n",
    "\n",
    "    print(clustData1.head(n=1))\n",
    "    print(clustData1.shape)\n",
    "    #clustData1.to_csv(destination+\"/clustData_\"+lang+\".csv\",index=False)\n",
    "    ##clustData.groupby([\"chCategory\",\"clustNo\",\"v_title\"]).size().sort_values(ascending=False)\n",
    "    #print(clustData1.columns)\n",
    "    return clustData1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33808, 32)\n",
      "END TIME after TFIDF = 2018-07-05 10:56:47.583108\n",
      "time taken until TFIDF= 0:01:15.431852\n",
      "cluster lang= telugu\n",
      "clust data len= 33808\n",
      "cluster size= 19\n",
      "Counter({0: 14996, 11: 3577, 5: 1625, 2: 799, 10: 339, 12: 265, 18: 157, 13: 114, 16: 61, 6: 47, 1: 41, 4: 26, 14: 21, 17: 20, 3: 19, 7: 17, 9: 17, 15: 12, 8: 8})\n",
      "                v_id                                            v_title  \\\n",
      "987  144531536255932  mrigiNcin yaalukltoo vaarN loonee mimmlni miir...   \n",
      "\n",
      "                          v_title_trans ch_lang_raw  Unnamed: 0  ch_id  \\\n",
      "987  mrigiNcc in the world of your life      telugu         NaN    NaN   \n",
      "\n",
      "    v_descr  v_seq_id v_lang_raw v_thumbnail   ...     v_stat_viewCnt  \\\n",
      "987     NaN       NaN        NaN         NaN   ...                NaN   \n",
      "\n",
      "    v_stat_likeCnt v_stat_dislikeCnt  v_stat_cmntCnt v_stat_status  \\\n",
      "987            NaN               NaN             NaN           NaN   \n",
      "\n",
      "    v_stat_updatStamp lang_vtitle videoPubDate videoPubYr clustNum  \n",
      "987               NaN         NaN          NaN        NaN     10.0  \n",
      "\n",
      "[1 rows x 33 columns]\n",
      "(33808, 33)\n",
      "(33808, 33)\n",
      "TIME taken for Modeling= 0:01:18.634347\n"
     ]
    }
   ],
   "source": [
    "## LANGUAGE LEVEL CLUSTERING\n",
    "startT = datetime.datetime.now()\n",
    "#langs = ['english','hindi','bengali','telugu','tamil']\n",
    "langs = ['telugu']\n",
    "for lang in langs:\n",
    "        outData5 = outData43[outData43.ch_lang_raw==lang]\n",
    "        ## Dataframe Length Check\n",
    "        if(len(outData5)>0):\n",
    "            print(outData5.shape)\n",
    "            \n",
    "            ## feature engineering\n",
    "            ##feature_vecto = getTfIdfMetrics(outData5,lang)\n",
    "            feature_vecto = getWord2VecMetrics(outData5,lang)\n",
    "            ## Model Data Function Call\n",
    "            clustData2 = getModelDataTfIdf(outData5,lang,feature_vecto)\n",
    "            print(clustData2.shape)\n",
    "endT = datetime.datetime.now()\n",
    "print(\"TIME taken for Modeling=\",(endT-startT))\n",
    "## 160 records - 36 min time with tfidf kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ## WORD2VEC model # Word2Vec\n",
    "# def getWord2VecMetrics(outData44,lang):\n",
    "# outData44_en = outData44[outData44.ch_lang_raw==lang]\n",
    "# t0 = datetime.datetime.now()\n",
    "# cleaneddoc = [nltk_extract_tags(text) for text in list(outData44_en[\"v_title_trans\"])]\n",
    "# cleaneddoc = [text for text in cleaneddoc if len(text)>0]\n",
    "# w2vmodel = Word2Vec(cleaneddoc,min_count=1,size=200)\n",
    "# w2v_vecs = w2vmodel.wv.syn0\n",
    "# t1 = datetime.datetime.now()\n",
    "# print(\"END TIME after w2v =\",t1)\n",
    "# print(\"time taken until w2v=\",(t1-t0))\n",
    "# return w2v_vecs, w2vmodel\n",
    "# # outData44.ch_lang_raw.unique()\n",
    "# feature_vecto , w2vmodel= getWord2VecMetrics(outData44,'english')\n",
    "# # print(feature_vecto.shape)\n",
    "\n",
    "# word_vectors = feature_vecto\n",
    "\n",
    "# import time\n",
    "\n",
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "# n_words = word_vectors.shape[0]\n",
    "# vec_size = word_vectors.shape[1]\n",
    "# print(\"#words = {0}, vector size = {1}\".format(n_words, vec_size))\n",
    "\n",
    "# start = time.time()\n",
    "# print(\"Compute clustering ... \", end=\"\", flush=True)\n",
    "# kmeans = KMeans(n_clusters=10, n_jobs=-1, random_state=0)\n",
    "# idx = kmeans.fit_predict(word_vectors)\n",
    "# print(\"finished in {:.2f} sec.\".format(time.time() - start), flush=True)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# inertia = []\n",
    "# for k in range(1, 20):\n",
    "# kmeans = KMeans(n_clusters=10, n_jobs=-1, random_state=0)\n",
    "# # idx = kmeans.fit_predict(word_vectors)\n",
    "\n",
    "# kmeans = KMeans(n_clusters=k, random_state=1).fit(word_vectors)\n",
    "# inertia.append(np.sqrt(kmeans.inertia_))\n",
    "\n",
    "# plt.plot(range(1, 20), inertia, marker='s');\n",
    "# plt.xlabel('$k$')\n",
    "# plt.ylabel('$J(C_k)$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#     from sklearn.cluster import k_means_\n",
    "#     from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "\n",
    "#     # Manually override euclidean\n",
    "#     def euc_dist(X, Y = None, Y_norm_squared = None, squared = False):\n",
    "#         #return pairwise_distances(X, Y, metric = 'cosine', n_jobs = 10)\n",
    "#         return cosine_similarity(X, Y)\n",
    "#     k_means_.euclidean_distances = euc_dist\n",
    "\n",
    "#     kmeans = k_means_.KMeans(n_clusters = clustSize,random_state = 3425).fit(tfidf_vecto)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
