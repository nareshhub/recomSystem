{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!pip install gensim\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cj-pythonrecsys/adviewRecomm/version3/temp/\n"
     ]
    }
   ],
   "source": [
    "## import data processing/cleaning , data modeling libraries\n",
    "#!pip install boto3\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import re as re\n",
    "import datetime as datetime\n",
    "import numpy as np\n",
    "import collections\n",
    "import boto3\n",
    "import io\n",
    "import string\n",
    "\n",
    "#from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import silhouette_score\n",
    "    \n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models import doc2vec\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import fasttext\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "source=\"/home/cj-pythonrecsys/adviewRecomm/version3/temp/\"\n",
    "sparkfiles = \"/home/cj-pythonrecsys/spark-modules/clust_vtitleTrans/\"\n",
    "destination=source\n",
    "\n",
    "lang=[\"en\"] # video language got from video title using python Library\n",
    "clustSize = 0 # no of clusters - initialization\n",
    "t0 = datetime.datetime.now()\n",
    "print(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(274299, 19)\n",
      "          v_id                                            v_title  \\\n",
      "0  YH6RzRnMVHE  Marigold Paper Flowers Designs | Easy Paper Cu...   \n",
      "1  _EOzjBv-_RI  Fun Comedy Videos | Telugu Comedy Videos | Fun...   \n",
      "\n",
      "                                       v_title_trans ch_lang_raw  v_seq_id  \\\n",
      "0  Marigold Paper Flowers Designs | Easy Paper Cu...       hindi   1238508   \n",
      "1  Fun Comedy Videos | Telugu Comedy Videos | Fun...       hindi   1238515   \n",
      "\n",
      "   v_duration                                            v_descr  \\\n",
      "0          94  Marigold Paper Flowers Designs | Easy Paper Cu...   \n",
      "1         108  Fun Comedy Videos | Telugu Comedy Videos | Fun...   \n",
      "\n",
      "                                        v_thumbnail  \\\n",
      "0  https://i.ytimg.com/vi/YH6RzRnMVHE/mqdefault.jpg   \n",
      "1  https://i.ytimg.com/vi/_EOzjBv-_RI/mqdefault.jpg   \n",
      "\n",
      "                                            ch_descr  ch_id       ch_name  \\\n",
      "0  Trying to define a city's atmosphere and energ...   1264  Madras Meter   \n",
      "1  Trying to define a city's atmosphere and energ...   1264  Madras Meter   \n",
      "\n",
      "                                        ch_thumbnail  v_categ_id  ch_active  \\\n",
      "0  https://yt3.ggpht.com/-zmqw2zxhSDI/AAAAAAAAAAI...          20       True   \n",
      "1  https://yt3.ggpht.com/-zmqw2zxhSDI/AAAAAAAAAAI...          20       True   \n",
      "\n",
      "   ch_is_clickable chSource  v_isdownloaded  \\\n",
      "0             True  youtube            True   \n",
      "1             True  youtube            True   \n",
      "\n",
      "                                             v_alias      ch_alias  \n",
      "0  Marigold Paper Flowers Designs | Easy Paper Cu...  Madras Meter  \n",
      "1  Fun Comedy Videos | Telugu Comedy Videos | Fun...  Madras Meter  \n",
      "(274299, 19)\n"
     ]
    }
   ],
   "source": [
    "## outData41 - video data coming from postgres DB\n",
    "#  outData41.to_csv(source+\"youtubeModelData_stats1.csv\",index=False)\n",
    "\n",
    "outData43 = pd.read_csv(source+\"clipVdChanelData_model_in.csv\",encoding=\"utf-8\")\n",
    "print(outData43.shape)\n",
    "print(outData43.head(n=2)) # 39:\n",
    "outData43 = outData43.drop_duplicates()\n",
    "outData43 = outData43.dropna(subset=[\"v_title_trans\"])\n",
    "print(outData43.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['title',\n",
       " 'khan',\n",
       " 'singh',\n",
       " 'song',\n",
       " 'shah',\n",
       " 'sukhvinder',\n",
       " 'chak',\n",
       " 'rukh',\n",
       " 'salim',\n",
       " 'marianne',\n",
       " 'cruz',\n",
       " 'full',\n",
       " 'india']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## NLP PRE-PROCESSING\n",
    "\n",
    "#text=\" the JanaSena Party Formation Day celebrations || LIVE funny celebration|| Pawan Kalyan || Guntur\"\n",
    "text=\"Chak De India | Full Title Song | Shah Rukh Khan | Sukhvinder Singh | Salim | Marianne D'Cruz\"\n",
    "\n",
    "def nltk_clean_sent(line):\n",
    "    if len(line)>0:\n",
    "        ## remove the punctuation/emoticons/digits/multispaces with single from the line\n",
    "        ## dont make lowercase before the pos tagging\n",
    "        line_lower = line.strip()\n",
    "        line_punct = re.sub('['+string.punctuation+']',' ',line_lower)\n",
    "        line_emots = re.sub(r'[\\u200b-\\u2fff]+',' ',line_punct)\n",
    "        line_digis = re.sub(r'[0-9]+',' ',line_emots)\n",
    "        line_spaces = re.sub(r'[\\s]+',' ',line_digis)\n",
    "        line = line_spaces\n",
    "    return line\n",
    "\n",
    "def nltk_extract_postags(line):\n",
    "    cleaned_str = ''\n",
    "    tokens = nltk.word_tokenize(line)\n",
    "    tokens_pos = PerceptronTagger().tag(tokens)\n",
    "    #print(tokens_pos)\n",
    "    # noun tags\n",
    "    nn_tags = ['NN', 'NNP', 'NNP', 'NNPS', 'NNS']\n",
    "    # adjectives\n",
    "    jj_tags = ['JJ', 'JJR', 'JJS']\n",
    "    # verbs\n",
    "    vb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "    nltk_tags = nn_tags + jj_tags + vb_tags\n",
    "    \n",
    "    for tag_word in tokens_pos:\n",
    "        if tag_word[1] in nltk_tags:\n",
    "            if len(tag_word[0])>2:\n",
    "                cleaned_str += tag_word[0]+' '\n",
    "    return cleaned_str.strip().lower()\n",
    "\n",
    "def nltk_apply_lemma(line):\n",
    "    tokens_lemmas = [WordNetLemmatizer().lemmatize(word) for word in line.split()]\n",
    "    ## stemming\n",
    "    tokens_stops = [word for word in tokens_lemmas if word not in stopwords.words('english')]\n",
    "    tokens_stops = [word for word in tokens_stops if len(word.strip())>2]\n",
    "    tokens_stops = list(set(tokens_stops))\n",
    "    return tokens_stops\n",
    "\n",
    "def nltk_extract_tags(line):\n",
    "        ## tokenize the sentence/get tokens that contains only letters\n",
    "        line_clean = nltk_clean_sent(line)\n",
    "        ## apply postags to the words and get only couple of tags and word length >2\n",
    "        tokens_pos = nltk_extract_postags(line_clean)\n",
    "        ## apply lemmatize/stemming and remove stopwords\n",
    "        token_lemma = nltk_apply_lemma(tokens_pos)\n",
    "        return token_lemma\n",
    "    \n",
    "nltk_extract_tags(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=2519370, size=300, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "## FASTEXT\n",
    "# We distribute pre-trained word vectors for 157 languages, trained on Common Crawl and Wikipedia using fastText. \n",
    "# These models were trained using CBOW with position-weights, in dimension 300,with character n-grams of length 5, a window of size 5 and 10 negatives\n",
    "from gensim.models.wrappers import FastText\n",
    "\n",
    "model = FastText.load_fasttext_format('/home/fastextvecs/wiki.en.bin')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=0, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# from gensim.models.wrappers import FastText\n",
    "# print(FastText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          v_id                                            v_title  \\\n",
      "0  YH6RzRnMVHE  Marigold Paper Flowers Designs | Easy Paper Cu...   \n",
      "1  _EOzjBv-_RI  Fun Comedy Videos | Telugu Comedy Videos | Fun...   \n",
      "\n",
      "                                       v_title_trans ch_lang_raw  v_seq_id  \\\n",
      "0  Marigold Paper Flowers Designs | Easy Paper Cu...       hindi   1238508   \n",
      "1  Fun Comedy Videos | Telugu Comedy Videos | Fun...       hindi   1238515   \n",
      "\n",
      "   v_duration                                            v_descr  \\\n",
      "0          94  Marigold Paper Flowers Designs | Easy Paper Cu...   \n",
      "1         108  Fun Comedy Videos | Telugu Comedy Videos | Fun...   \n",
      "\n",
      "                                        v_thumbnail  \\\n",
      "0  https://i.ytimg.com/vi/YH6RzRnMVHE/mqdefault.jpg   \n",
      "1  https://i.ytimg.com/vi/_EOzjBv-_RI/mqdefault.jpg   \n",
      "\n",
      "                                            ch_descr  ch_id  \\\n",
      "0  Trying to define a city's atmosphere and energ...   1264   \n",
      "1  Trying to define a city's atmosphere and energ...   1264   \n",
      "\n",
      "                         ...                          \\\n",
      "0                        ...                           \n",
      "1                        ...                           \n",
      "\n",
      "                                        ch_thumbnail v_categ_id  ch_active  \\\n",
      "0  https://yt3.ggpht.com/-zmqw2zxhSDI/AAAAAAAAAAI...         20       True   \n",
      "1  https://yt3.ggpht.com/-zmqw2zxhSDI/AAAAAAAAAAI...         20       True   \n",
      "\n",
      "   ch_is_clickable  chSource v_isdownloaded  \\\n",
      "0             True   youtube           True   \n",
      "1             True   youtube           True   \n",
      "\n",
      "                                             v_alias      ch_alias  \\\n",
      "0  Marigold Paper Flowers Designs | Easy Paper Cu...  Madras Meter   \n",
      "1  Fun Comedy Videos | Telugu Comedy Videos | Fun...  Madras Meter   \n",
      "\n",
      "                                   clean_title_words  \\\n",
      "0  [best, marigold, paper, diy, cutting, idea, ea...   \n",
      "1   [video, medium, telugu, gama, tube, comedy, fun]   \n",
      "\n",
      "                                    clean_title_text  \n",
      "0  best marigold paper diy cutting idea easy desi...  \n",
      "1           video medium telugu gama tube comedy fun  \n",
      "\n",
      "[2 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# Compute clean title\n",
    "outData43['clean_title_words'] = outData43.v_title_trans.apply(nltk_extract_tags)\n",
    "#print(df_en.head(n=2))\n",
    "outData43['clean_title_text'] = outData43.clean_title_words.apply(lambda x:' '.join(x))\n",
    "print(outData43.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TF-IDF transformation\n",
    "def getTfIdfMetrics(outData44,lang):\n",
    "    outData44_en = outData44[outData44.ch_lang_raw==lang]\n",
    "    t0 = datetime.datetime.now()\n",
    "    tfidf_transform = TfidfVectorizer(tokenizer=nltk_extract_tags,min_df=6,max_df=0.95,stop_words='english',use_idf=True,ngram_range=(1,1))\n",
    "    #terms = tfidf_transform.get_feature_names()\n",
    "    #print(terms[:100])\n",
    "    tfidf_vecto = tfidf_transform.fit_transform(outData44_en[\"clean_title_text\"])\n",
    "    tf_idf_dict = dict(zip(tfidf_transform.get_feature_names(),tfidf_transform.idf_))\n",
    "    print(tfidf_vecto.shape)\n",
    "    # got shape of (83806, 149542) - without translation\n",
    "    t1 = datetime.datetime.now()\n",
    "    print(\"END TIME after TFIDF =\",t1)\n",
    "    print(\"time taken until TFIDF=\",(t1-t0))\n",
    "    return tf_idf_dict\n",
    "\n",
    "## PRINTING FIRST 100 FEATURES\n",
    "#getTfIdfMetrics(outData44,'telugu')\n",
    "#terms = tfidf_transform.get_feature_names()\n",
    "#print(terms[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embed_size = 300\n",
    "\n",
    "def sentences_train_w_tf_idf(strings):\n",
    "\t# Compute the embedding of a sentence to a 300 dimension vector\n",
    "\t# Mix tf-idf with FastText on each word to resume to a vector the sentence\n",
    "    #print(len(tf_idf_dict))\n",
    "    len_string = len(strings)\n",
    "    words = np.zeros((len_string,embed_size))\n",
    "    w = np.zeros(len_string)\n",
    "    for i,word in enumerate(strings):\n",
    "        try:\n",
    "            if word in tf_idf_dict.keys():\n",
    "                words[i]=model.wv[word]\n",
    "                words[i]=words[i]*tf_idf_dict[word]\n",
    "        except:\n",
    "            words[i]=np.random.rand(embed_size)\n",
    "            #print(\"not found in fasttext\")\n",
    "    finalvec = np.sum(words,0)/len(words)\n",
    "    return finalvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## MODEL FUNCTION DEFINITION\n",
    "#feature_vecto = np.stack(d2vmodel.docvecs)\n",
    "def getModelDataTfIdf(outData5,lang,xtrain):\n",
    "    ## chosing no of clusters\n",
    "    if len(outData5) < 1600:\n",
    "        clustSize=3\n",
    "    else:\n",
    "        clustSize = int(np.round(np.divide(len(outData5),1600)))\n",
    "    print(\"cluster lang=\",lang)    \n",
    "    print(\"clust data len=\",len(outData5))\n",
    "    print(\"cluster size=\",clustSize)\n",
    "\n",
    "    ## KMeans model\n",
    "    from sklearn.cluster import KMeans\n",
    "    #kmeans = KMeans(n_clusters=clustSize,init='k-means++',max_iter=80).fit(feature_vecto)\n",
    "    kmeans = KMeans(n_clusters=clustSize).fit(xtrain)\n",
    "    #kmeans = GaussianMixture(n_components=clustSize).fit(xtrain)\n",
    "    preds1 = kmeans.labels_\n",
    "\n",
    "    ### NO worries on this PART ###\n",
    "    print(collections.Counter(preds1))\n",
    "    b=pd.DataFrame(preds1)\n",
    "    clustData1=pd.DataFrame(outData5)\n",
    "    clustData1[\"clustNum\"]=pd.Series(preds1).values\n",
    "\n",
    "    #print(clustData1.head(n=1))\n",
    "    #print(clustData1.shape)\n",
    "    #clustData1.to_csv(destination+\"/clustData_\"+lang+\".csv\",index=False)\n",
    "    ##clustData.groupby([\"chCategory\",\"clustNo\",\"v_title\"]).size().sort_values(ascending=False)\n",
    "    #print(clustData1.columns)\n",
    "    return clustData1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32639, 21)\n",
      "(32639, 4788)\n",
      "END TIME after TFIDF = 2018-09-05 16:59:36.294668\n",
      "time taken until TFIDF= 0:00:57.469035\n",
      "4788\n",
      "xtrain shape= (32639, 300)\n",
      "outData5 shape= (32639, 22)\n",
      "cluster lang= bengali\n",
      "clust data len= 32639\n",
      "cluster size= 20\n",
      "Counter({1: 4391, 10: 3436, 3: 3022, 15: 2505, 7: 2448, 18: 2014, 5: 1995, 14: 1863, 6: 1617, 4: 1616, 8: 1481, 0: 1443, 16: 1357, 9: 960, 11: 960, 12: 741, 2: 617, 19: 78, 13: 58, 17: 37})\n",
      "final data= (32639, 23)\n",
      "Index(['v_id', 'v_title', 'v_title_trans', 'ch_lang_raw', 'v_seq_id',\n",
      "       'v_duration', 'v_descr', 'v_thumbnail', 'ch_descr', 'ch_id', 'ch_name',\n",
      "       'ch_thumbnail', 'v_categ_id', 'ch_active', 'ch_is_clickable',\n",
      "       'chSource', 'v_isdownloaded', 'v_alias', 'ch_alias',\n",
      "       'clean_title_words', 'clean_title_text', 'vectors_word2vec_tf_idf',\n",
      "       'clustNum'],\n",
      "      dtype='object')\n",
      "(25982, 21)\n",
      "(25982, 3681)\n",
      "END TIME after TFIDF = 2018-09-05 17:05:08.685585\n",
      "time taken until TFIDF= 0:00:47.354092\n",
      "3681\n",
      "xtrain shape= (25982, 300)\n",
      "outData5 shape= (25982, 22)\n",
      "cluster lang= telugu\n",
      "clust data len= 25982\n",
      "cluster size= 16\n",
      "Counter({10: 3068, 6: 2956, 13: 2584, 5: 2050, 0: 1985, 2: 1972, 8: 1669, 15: 1580, 12: 1525, 4: 1228, 9: 1019, 1: 1012, 7: 1004, 14: 932, 11: 771, 3: 627})\n",
      "final data= (25982, 23)\n",
      "Index(['v_id', 'v_title', 'v_title_trans', 'ch_lang_raw', 'v_seq_id',\n",
      "       'v_duration', 'v_descr', 'v_thumbnail', 'ch_descr', 'ch_id', 'ch_name',\n",
      "       'ch_thumbnail', 'v_categ_id', 'ch_active', 'ch_is_clickable',\n",
      "       'chSource', 'v_isdownloaded', 'v_alias', 'ch_alias',\n",
      "       'clean_title_words', 'clean_title_text', 'vectors_word2vec_tf_idf',\n",
      "       'clustNum'],\n",
      "      dtype='object')\n",
      "(81093, 21)\n",
      "(81093, 8037)\n",
      "END TIME after TFIDF = 2018-09-05 17:10:32.170699\n",
      "time taken until TFIDF= 0:02:17.836883\n",
      "8037\n",
      "xtrain shape= (81093, 300)\n",
      "outData5 shape= (81093, 22)\n",
      "cluster lang= tamil\n",
      "clust data len= 81093\n",
      "cluster size= 51\n",
      "Counter({25: 5838, 30: 4474, 3: 4014, 33: 3781, 45: 3363, 29: 3278, 5: 3023, 12: 2609, 0: 2552, 49: 2410, 11: 2283, 37: 2170, 23: 2152, 13: 2133, 15: 2119, 43: 2003, 17: 1923, 18: 1913, 8: 1866, 24: 1845, 26: 1843, 41: 1822, 10: 1758, 14: 1452, 35: 1417, 38: 1402, 16: 1344, 7: 1333, 31: 1219, 28: 1050, 32: 1028, 6: 1013, 39: 948, 20: 905, 1: 863, 2: 743, 34: 742, 27: 701, 36: 672, 19: 625, 44: 560, 22: 412, 42: 398, 47: 327, 46: 318, 48: 165, 4: 125, 50: 102, 9: 24, 40: 23, 21: 10})\n",
      "final data= (81093, 23)\n",
      "Index(['v_id', 'v_title', 'v_title_trans', 'ch_lang_raw', 'v_seq_id',\n",
      "       'v_duration', 'v_descr', 'v_thumbnail', 'ch_descr', 'ch_id', 'ch_name',\n",
      "       'ch_thumbnail', 'v_categ_id', 'ch_active', 'ch_is_clickable',\n",
      "       'chSource', 'v_isdownloaded', 'v_alias', 'ch_alias',\n",
      "       'clean_title_words', 'clean_title_text', 'vectors_word2vec_tf_idf',\n",
      "       'clustNum'],\n",
      "      dtype='object')\n",
      "(29890, 21)\n",
      "(29890, 3909)\n",
      "END TIME after TFIDF = 2018-09-05 17:24:32.333550\n",
      "time taken until TFIDF= 0:00:44.785842\n",
      "3909\n",
      "xtrain shape= (29890, 300)\n",
      "outData5 shape= (29890, 22)\n",
      "cluster lang= english\n",
      "clust data len= 29890\n",
      "cluster size= 19\n",
      "Counter({0: 6196, 16: 3373, 4: 2808, 2: 2279, 9: 2097, 18: 2097, 13: 1787, 3: 1658, 12: 1198, 11: 1186, 7: 992, 1: 920, 8: 915, 10: 896, 14: 559, 5: 443, 17: 383, 6: 80, 15: 23})\n",
      "final data= (29890, 23)\n",
      "Index(['v_id', 'v_title', 'v_title_trans', 'ch_lang_raw', 'v_seq_id',\n",
      "       'v_duration', 'v_descr', 'v_thumbnail', 'ch_descr', 'ch_id', 'ch_name',\n",
      "       'ch_thumbnail', 'v_categ_id', 'ch_active', 'ch_is_clickable',\n",
      "       'chSource', 'v_isdownloaded', 'v_alias', 'ch_alias',\n",
      "       'clean_title_words', 'clean_title_text', 'vectors_word2vec_tf_idf',\n",
      "       'clustNum'],\n",
      "      dtype='object')\n",
      "(104695, 21)\n",
      "(104695, 10851)\n",
      "END TIME after TFIDF = 2018-09-05 17:31:33.284200\n",
      "time taken until TFIDF= 0:03:03.538679\n",
      "10851\n",
      "xtrain shape= (104695, 300)\n",
      "outData5 shape= (104695, 22)\n",
      "cluster lang= hindi\n",
      "clust data len= 104695\n",
      "cluster size= 65\n",
      "Counter({10: 5857, 32: 4266, 39: 4145, 4: 3346, 47: 3142, 55: 3058, 25: 2981, 18: 2701, 19: 2672, 1: 2651, 22: 2530, 50: 2511, 54: 2508, 62: 2495, 60: 2361, 37: 2307, 40: 2255, 9: 2249, 42: 2222, 12: 2136, 33: 2127, 57: 2073, 48: 2060, 15: 1848, 61: 1785, 45: 1778, 52: 1738, 3: 1668, 34: 1636, 2: 1601, 24: 1494, 41: 1444, 14: 1410, 31: 1406, 16: 1317, 5: 1227, 58: 1210, 56: 1190, 8: 1171, 23: 1131, 7: 1061, 17: 1053, 27: 1042, 59: 1017, 49: 1011, 6: 1008, 13: 892, 35: 823, 36: 779, 29: 732, 21: 701, 11: 696, 43: 592, 28: 585, 26: 535, 63: 529, 51: 519, 38: 339, 30: 335, 64: 253, 44: 135, 20: 128, 0: 106, 53: 90, 46: 27})\n",
      "final data= (104695, 23)\n",
      "Index(['v_id', 'v_title', 'v_title_trans', 'ch_lang_raw', 'v_seq_id',\n",
      "       'v_duration', 'v_descr', 'v_thumbnail', 'ch_descr', 'ch_id', 'ch_name',\n",
      "       'ch_thumbnail', 'v_categ_id', 'ch_active', 'ch_is_clickable',\n",
      "       'chSource', 'v_isdownloaded', 'v_alias', 'ch_alias',\n",
      "       'clean_title_words', 'clean_title_text', 'vectors_word2vec_tf_idf',\n",
      "       'clustNum'],\n",
      "      dtype='object')\n",
      "TIME taken for Modeling= 0:50:40.903911\n"
     ]
    }
   ],
   "source": [
    "## LANGUAGE LEVEL CLUSTERING\n",
    "startT = datetime.datetime.now()\n",
    "langs = ['bengali','telugu','tamil','english','hindi']\n",
    "#langs = ['english']\n",
    "for lang in langs:\n",
    "        outData5 = outData43[outData43.ch_lang_raw==lang]\n",
    "        ## Dataframe Length Check\n",
    "        if(len(outData5)>0):\n",
    "            print(outData5.shape)\n",
    "            \n",
    "            ## feature engineering\n",
    "            tf_idf_dict = getTfIdfMetrics(outData5,lang)\n",
    "            print(len(tf_idf_dict))\n",
    "            outData5['vectors_word2vec_tf_idf'] = outData5.clean_title_words.apply(sentences_train_w_tf_idf)\n",
    "            xtrain = outData5.vectors_word2vec_tf_idf.apply(lambda x: pd.Series(list(x)))\n",
    "            print(\"xtrain shape=\",xtrain.shape)\n",
    "            #print(xtrain.head(n=2))\n",
    "            xtrain= xtrain.round(2)\n",
    "            xtrain.fillna(xtrain.mean(),inplace=True)\n",
    "            #print(xtrain.info())\n",
    "            #xtrain=xtrain.replace([np.inf, -np.inf],0)\n",
    "            print(\"outData5 shape=\",outData5.shape)\n",
    "#             #feature_vecto = getWord2VecMetrics(outData5,lang)\n",
    "#             ## Model Data Function Call\n",
    "            clustData2 = getModelDataTfIdf(outData5,lang,xtrain)\n",
    "            print(\"final data=\",clustData2.shape)\n",
    "            print(clustData2.columns)\n",
    "            clustData2.to_csv(destination+\"/clustData_\"+lang+\".csv\",index=False)\n",
    "endT = datetime.datetime.now()\n",
    "print(\"TIME taken for Modeling=\",(endT-startT))\n",
    "## 50 min for modelling - 274K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(clustData2.shape)\n",
    "# clustData21 = clustData2.drop_duplicates(subset=['v_id'])\n",
    "# print(clustData21.shape)\n",
    "# clustData21 = clustData21[clustData21.clustNum==2]\n",
    "# print(clustData21.shape)\n",
    "# print(clustData21.head(n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## sentence vector representation\n",
    "\n",
    "# def sent_vectorizer(sent, model):\n",
    "#     sent_vec = np.zeros(400)\n",
    "#     numw = 0\n",
    "#     for w in sent:\n",
    "#         try:\n",
    "#             sent_vec = np.add(sent_vec, model[w])\n",
    "#             numw+=1\n",
    "#         except:\n",
    "#             pass\n",
    "#     return sent_vec / np.sqrt(sent_vec.dot(sent_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#     from sklearn.cluster import k_means_\n",
    "#     from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "\n",
    "#     # Manually override euclidean\n",
    "#     def euc_dist(X, Y = None, Y_norm_squared = None, squared = False):\n",
    "#         #return pairwise_distances(X, Y, metric = 'cosine', n_jobs = 10)\n",
    "#         return cosine_similarity(X, Y)\n",
    "#     k_means_.euclidean_distances = euc_dist\n",
    "\n",
    "#     kmeans = k_means_.KMeans(n_clusters = clustSize,random_state = 3425).fit(tfidf_vecto)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
